# Information Entropy
信息熵

<br />

## 概述
信息熵（***Information Entropy***）定义为：由一个随机（[*stochastic*](https://en.wikipedia.org/wiki/Stochastic)）数据源所产生的平均信息量。
对每个可能的数据值所关联的信息熵的测量是该值的概率质量函数的负对数。因而，当数据源具有一个较低概率的值时（也就是当一个低概率事件发生时），该事件会携带更多的“信息”；相比之下，如果源数据具有一个较高概率的值时，那么它携带了更少的“信息”。以这种方式而定义的由每个事件所表达的信息量变为一个随机变量，其期望值就是**信息熵**。一般来说，***熵***引用为无序性与不确定性，并且在信息论中所使用的熵的定义很直接的类同于统计热力学中所使用的定义。

![两枚硬币的熵](https://github.com/zenny-chen/Information-Entropy/blob/master/1.jpg)

以上两个硬币的图描述了两个比特位的熵。因为一枚硬币有正反两个面，每个面表示这枚硬币的一个值，因此每枚硬币正好有两个值，正好可以用一个二进制比特位来表示。我们对一枚均匀的硬币进行两次投掷，基于比特的信息熵就是以2为底数，可能产生出结果的个数的对数。对于两枚硬币，则可能产生出4种结果以及2个比特的熵。一般来说，信息熵就是一个事件所表达的信息量，并且考虑该事件所有可能产生出的结果。
对于熵的测量使用概率分布的对数是很有用的，因为它对于各个独立的数据源是可叠加的。比如，对一枚均匀硬币的投掷1次的熵是1比特，那么对它投掷*m*次就是*m*比特。用直接的方式来表示就是，需要log<sub>2</sub>(*n*)个比特位来表示可以取*n*个值的一个变量，如果*n*正好是2的幂。如果这些值的发生概率是相同的，那么（基于比特的）熵就等于log<sub>2</sub>(*n*)。如果其中一个值的发生概率较大，那么对该值发生的观察所需的信息量就较少。由于较小概率的事件发生更少，因而有效效应为从非均匀分布的数据所接受到的熵（认作为平均信息）总是小于等于log<sub>2</sub>(*n*)。当一个产生结果确定发生时，熵为零。熵量化了当源数据的概率分布是已知时的要考虑的事情。所观察事件的*意义*（消息的*意义*）对于熵的定义没有任何关系。熵仅仅考虑观察一个特定事件的概率，因此它所封装的信息是关于底层概率分布的信息，而不是事件自身的意义。

## 介绍
熵是对状态不确定性的测量，或者说是对平均信息内容的测量。要获得对这些术语在直觉上的理解，我们可以考虑一个政治民意调查的例子。通常，要做这些调查投票是因为投票的产出结果还是未知的。换句话说，投票的产出结果相对是不可预测的，而实际进行的投票而获得的结果是给出了某些新的*信息*；这些只是以不同的方式来说明投票结果的*先验*熵是大的。现在我们来考虑这么一种情况，当第一次投票完成之后立即执行同样的第二次投票。由于第一次投票结果是已知的，因而第二次投票的结果可以被很好地预测，并且投票结果不应该包含更多新的信息；在这种情况下，第二次投票的先验熵相对于第一次就会变小。
现在考虑投掷硬币的例子。假定正面的概率与反面的概率相同，那么投掷硬币的熵为最大值。这是因为我们无法预测投掷硬币的结果：如果我们要进行选择的话，我们所能做的最佳预测就是硬币会面朝上，而这个预测正确的概率为1/2。这么一次硬币投掷具有一个比特的熵，由于两次投掷的可能结果具有相同的概率，所以实际获得的结果也就包含一个比特的信息。相比之下，使用一枚硬币做投掷两次都是正面超上，没有反面朝上的情况，那么熵为零，由于硬币总是正面朝上，而结果都能完美地预测出。总结来说，具有等概率的一个二值产出结果具有 log<sub>2</sub>2 = 1比特的香农熵。类似地，具有等概率的三值产出结果具有 log<sub>2</sub>3（大约1.58496）比特的信息，因为该结果可能具有三个值中的其中一个。

<br />

## 其他相关文章

1. [信息熵是什么？](https://www.zhihu.com/question/22178202/answer/49929786)

